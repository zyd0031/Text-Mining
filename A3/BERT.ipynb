{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7274284,"sourceType":"datasetVersion","datasetId":4217180},{"sourceId":7274412,"sourceType":"datasetVersion","datasetId":4217260},{"sourceId":7332353,"sourceType":"datasetVersion","datasetId":4256515},{"sourceId":7274376,"sourceType":"datasetVersion","datasetId":4217242},{"sourceId":7274486,"sourceType":"datasetVersion","datasetId":4217295},{"sourceId":7274490,"sourceType":"datasetVersion","datasetId":4217299}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-25T00:04:15.047947Z","iopub.status.busy":"2023-12-25T00:04:15.047574Z","iopub.status.idle":"2023-12-25T00:04:15.079049Z","shell.execute_reply":"2023-12-25T00:04:15.077797Z","shell.execute_reply.started":"2023-12-25T00:04:15.047916Z"}},"execution_count":107,"outputs":[{"name":"stdout","output_type":"stream","text":"/kaggle/input/twitter-2016train-a/twitter-2016train-A.tsv\n\n/kaggle/input/twitter/twitter-2016test-A.tsv\n\n/kaggle/input/twitter-dev/twitter-2016dev-A.tsv\n"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:50:24.415448Z","iopub.execute_input":"2024-01-04T11:50:24.415796Z","iopub.status.idle":"2024-01-04T11:50:24.421892Z","shell.execute_reply.started":"2024-01-04T11:50:24.415761Z","shell.execute_reply":"2024-01-04T11:50:24.420929Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:50:26.077009Z","iopub.execute_input":"2024-01-04T11:50:26.077777Z","iopub.status.idle":"2024-01-04T11:50:30.048528Z","shell.execute_reply.started":"2024-01-04T11:50:26.077717Z","shell.execute_reply":"2024-01-04T11:50:30.047548Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_tweet(tweet):\n    if tweet is None or not isinstance(tweet, str):\n        return \"\"\n    \n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n    tweet = re.sub(r'#', '', tweet)\n    tweet = re.sub(r'@', '', tweet)\n    tweet = re.sub(r'&[a-z]+;', '', tweet)\n\n    emoticons = {\n        \":)\": \"<smile>\",\n        \":(\": \"<sadface>\",\n        \":D\": \"<lolface>\",\n        \":-)\": \"<smile>\",\n        \":-(\": \"<sadface>\",\n        \":-D\": \"<lolface>\",\n        \";)\": \"<wink>\",\n        \";(\": \"<sadface>\",\n        \";D\": \"<lolface>\",\n        \";-)\": \"<wink>\",\n        \";-(\": \"<sadface>\",\n        \";-D\": \"<lolface>\"\n    }\n    \n    for emoticon, replacement in emoticons.items():\n        tweet = tweet.replace(emoticon, replacement)\n\n    tweet = tweet.lower()\n\n    tweet = tweet.replace(\" u \", \" you \")\n\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(tweet)\n    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n\n    return ' '.join(filtered_tweet)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:50:31.234524Z","iopub.execute_input":"2024-01-04T11:50:31.235432Z","iopub.status.idle":"2024-01-04T11:50:31.244379Z","shell.execute_reply.started":"2024-01-04T11:50:31.235397Z","shell.execute_reply":"2024-01-04T11:50:31.243032Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class TwitterSentimentDataset(Dataset):\n    def __init__(self, tweets, labels, tokenizer):\n        self.tweets = tweets\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.tweets)\n\n    def __getitem__(self, item):\n        tweet = preprocess_tweet(str(self.tweets[item]))\n        label = int(self.labels[item])\n\n        encoding = self.tokenizer.encode_plus(\n            tweet,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            padding=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n            \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:50:37.287949Z","iopub.execute_input":"2024-01-04T11:50:37.288304Z","iopub.status.idle":"2024-01-04T11:50:37.296629Z","shell.execute_reply.started":"2024-01-04T11:50:37.288275Z","shell.execute_reply":"2024-01-04T11:50:37.295652Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:50:42.453058Z","iopub.execute_input":"2024-01-04T11:50:42.453425Z","iopub.status.idle":"2024-01-04T11:50:43.446095Z","shell.execute_reply.started":"2024-01-04T11:50:42.453392Z","shell.execute_reply":"2024-01-04T11:50:43.445374Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131c72718a934a678d60929039d6817d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec43b364c48413bb9b3762aeb78f7fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2b950d7b4e4966907f732ceb45b841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904a66dc37434cdfa255cc8ea3248464"}},"metadata":{}}]},{"cell_type":"code","source":"def load_data(filename):\n    df = pd.read_csv(filename, sep='\\t', header=None, names=['id', 'sentiment', 'tweet'])\n    df['sentiment'] = df['sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n    df.dropna(subset=['tweet', 'sentiment'], inplace=True)\n    df['tweet'] = df['tweet'].apply(preprocess_tweet)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:50:45.395628Z","iopub.execute_input":"2024-01-04T11:50:45.395991Z","iopub.status.idle":"2024-01-04T11:50:45.401623Z","shell.execute_reply.started":"2024-01-04T11:50:45.395962Z","shell.execute_reply":"2024-01-04T11:50:45.400645Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_train = load_data('/kaggle/input/twitter-2016train-a/twitter-2016train-A.tsv')\ndf_test = load_data('/kaggle/input/twitter-2016test/twitter-2016test-A.tsv')\ndf_val = load_data('/kaggle/input/twitter-dev/twitter-2016dev-A.tsv')\n\ntrain_datasets = TwitterSentimentDataset(df_train['tweet'].to_numpy(), df_train['sentiment'].to_numpy(), tokenizer)\ntest_datasets = TwitterSentimentDataset(df_test['tweet'].to_numpy(), df_test['sentiment'].to_numpy(), tokenizer)\nval_datasets = TwitterSentimentDataset(df_val['tweet'].to_numpy(), df_val['sentiment'].to_numpy(), tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:52:40.826920Z","iopub.execute_input":"2024-01-04T11:52:40.827679Z","iopub.status.idle":"2024-01-04T11:52:54.549674Z","shell.execute_reply.started":"2024-01-04T11:52:40.827649Z","shell.execute_reply":"2024-01-04T11:52:54.548890Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(val_datasets)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:52:57.152615Z","iopub.execute_input":"2024-01-04T11:52:57.152965Z","iopub.status.idle":"2024-01-04T11:52:57.159143Z","shell.execute_reply.started":"2024-01-04T11:52:57.152938Z","shell.execute_reply":"2024-01-04T11:52:57.157999Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"1966"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:53:10.295634Z","iopub.execute_input":"2024-01-04T11:53:10.296478Z","iopub.status.idle":"2024-01-04T11:53:20.456036Z","shell.execute_reply.started":"2024-01-04T11:53:10.296441Z","shell.execute_reply":"2024-01-04T11:53:20.454945Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:53:28.740485Z","iopub.execute_input":"2024-01-04T11:53:28.741121Z","iopub.status.idle":"2024-01-04T11:53:31.792867Z","shell.execute_reply.started":"2024-01-04T11:53:28.741086Z","shell.execute_reply":"2024-01-04T11:53:31.791941Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3077ac684b947e2aa476c9be1d6494e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'accuracy': acc,\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:53:33.561672Z","iopub.execute_input":"2024-01-04T11:53:33.562232Z","iopub.status.idle":"2024-01-04T11:53:33.570592Z","shell.execute_reply.started":"2024-01-04T11:53:33.562197Z","shell.execute_reply":"2024-01-04T11:53:33.569500Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(train_datasets[0])\nprint(test_datasets[0])\nprint(val_datasets[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T11:53:35.676676Z","iopub.execute_input":"2024-01-04T11:53:35.677240Z","iopub.status.idle":"2024-01-04T11:53:35.694426Z","shell.execute_reply.started":"2024-01-04T11:53:35.677190Z","shell.execute_reply":"2024-01-04T11:53:35.693567Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([  101,  6203,  7513,  2047, 21511,  8873,  3401,  6097,  2307,  1010,\n         1048,  6038,  2278, 10651,  1029,  1039,  1005, 12256,  1012,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n{'input_ids': tensor([  101,  3861,  4580,  1005,  1055,  1010,  5061, 12305,  1005,  1055,\n         1010,  1005,  5074,  5380,  1024,  2813,  2140,  1011,  3098,  2756,\n        17419,  2437,  5975,  1012,  3422,  9117,  5291,  2962,  1011,  2298,\n         1012,  1012,  1012,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(1)}\n{'input_ids': tensor([  101,  5709,  3786,  1011,  2745,  4027,  1011, 10874,  1006, 10965,\n         5315,  3179,  1007,  1031, 10751,  1033,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(1)}\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"my_baseline_model\",\n    evaluation_strategy=\"epoch\",\n    num_train_epochs=1,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_datasets,\n    eval_dataset=test_datasets,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\ntrainer.save_model(\"my_baseline_model\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:16:16.807594Z","iopub.execute_input":"2024-01-04T13:16:16.808494Z","iopub.status.idle":"2024-01-04T13:19:20.823190Z","shell.execute_reply.started":"2024-01-04T13:16:16.808456Z","shell.execute_reply":"2024-01-04T13:19:20.821919Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [367/367 03:01, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.040679</td>\n      <td>0.622638</td>\n      <td>0.609345</td>\n      <td>0.601023</td>\n      <td>0.609345</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"def model_init():\n    return BertForSequenceClassification.from_pretrained(\"my_baseline_model\")\n\nlearning_rates = [1e-5, 3e-5, 5e-5]\nbatch_sizes = [8, 16]\n\nresults = {}\nbest_loss = float(\"inf\")\nbest_model = None\n\nfor lr in learning_rates:\n    for batch_size in batch_sizes:\n        training_args = TrainingArguments(\n            output_dir=f'./results/lr_{lr}_bs_{batch_size}',\n            learning_rate=lr,\n            per_device_train_batch_size=batch_size,\n            num_train_epochs=3,\n            evaluation_strategy=\"epoch\",\n        )\n\n        trainer = Trainer(\n            model_init=model_init,\n            args=training_args,\n            train_dataset=train_datasets,\n            eval_dataset=val_datasets,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n            compute_metrics=compute_metrics,\n        )\n\n        trainer.train()\n\n        eval_result = trainer.evaluate()\n        eval_loss = eval_result[\"eval_loss\"]\n        results[f\"lr_{lr}_bs_{batch_size}\"] = eval_loss\n\n        if eval_loss < best_loss:\n            best_loss = eval_loss\n            best_model_dir = f\"./best_model/lr_{lr}_bs_{batch_size}\"\n            trainer.save_model(best_model_dir)\n            best_model = trainer.model\n\nfor key, value in results.items():\n    print(f\"{key}: Loss = {value}\")\n\nprint(f\"Best setup: {best_model_dir} with loss = {best_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:39:19.091144Z","iopub.execute_input":"2024-01-04T13:39:19.091956Z","iopub.status.idle":"2024-01-04T14:03:46.748613Z","shell.execute_reply.started":"2024-01-04T13:39:19.091920Z","shell.execute_reply":"2024-01-04T14:03:46.747213Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1101' max='1101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1101/1101 04:31, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.588631</td>\n      <td>0.582021</td>\n      <td>0.584435</td>\n      <td>0.582841</td>\n      <td>0.584435</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.201700</td>\n      <td>1.551923</td>\n      <td>0.584864</td>\n      <td>0.586470</td>\n      <td>0.582579</td>\n      <td>0.586470</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.209300</td>\n      <td>1.661520</td>\n      <td>0.586161</td>\n      <td>0.580366</td>\n      <td>0.579347</td>\n      <td>0.580366</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_1e-05_bs_8/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_1e-05_bs_8/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/123 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='552' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [552/552 03:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.271119</td>\n      <td>0.596751</td>\n      <td>0.585453</td>\n      <td>0.588589</td>\n      <td>0.585453</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.432721</td>\n      <td>0.587164</td>\n      <td>0.589013</td>\n      <td>0.586158</td>\n      <td>0.589013</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.207600</td>\n      <td>1.452635</td>\n      <td>0.583670</td>\n      <td>0.579349</td>\n      <td>0.577630</td>\n      <td>0.579349</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_1e-05_bs_16/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/123 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1101' max='1101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1101/1101 04:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.120221</td>\n      <td>0.571846</td>\n      <td>0.576297</td>\n      <td>0.573586</td>\n      <td>0.576297</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.238500</td>\n      <td>1.504399</td>\n      <td>0.583181</td>\n      <td>0.584435</td>\n      <td>0.575963</td>\n      <td>0.584435</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.195400</td>\n      <td>2.030499</td>\n      <td>0.587638</td>\n      <td>0.584435</td>\n      <td>0.582751</td>\n      <td>0.584435</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_3e-05_bs_8/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_3e-05_bs_8/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/123 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='552' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [552/552 03:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.144184</td>\n      <td>0.578103</td>\n      <td>0.568667</td>\n      <td>0.572092</td>\n      <td>0.568667</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.557032</td>\n      <td>0.583820</td>\n      <td>0.586470</td>\n      <td>0.580608</td>\n      <td>0.586470</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.200100</td>\n      <td>1.739992</td>\n      <td>0.594826</td>\n      <td>0.583418</td>\n      <td>0.583038</td>\n      <td>0.583418</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_3e-05_bs_16/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/123 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1101' max='1101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1101/1101 04:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.954095</td>\n      <td>0.570456</td>\n      <td>0.576806</td>\n      <td>0.568001</td>\n      <td>0.576806</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.286300</td>\n      <td>1.353040</td>\n      <td>0.579195</td>\n      <td>0.581384</td>\n      <td>0.576148</td>\n      <td>0.581384</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.209400</td>\n      <td>2.207650</td>\n      <td>0.574346</td>\n      <td>0.573245</td>\n      <td>0.569737</td>\n      <td>0.573245</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_5e-05_bs_8/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_5e-05_bs_8/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/123 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='552' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [552/552 03:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.045865</td>\n      <td>0.582929</td>\n      <td>0.570702</td>\n      <td>0.574895</td>\n      <td>0.570702</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.490745</td>\n      <td>0.580880</td>\n      <td>0.579858</td>\n      <td>0.577941</td>\n      <td>0.579858</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.228600</td>\n      <td>1.765489</td>\n      <td>0.585895</td>\n      <td>0.576806</td>\n      <td>0.575510</td>\n      <td>0.576806</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCheckpoint destination directory ./results/lr_5e-05_bs_16/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/123 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"lr_1e-05_bs_8: Loss = 1.66152024269104\nlr_1e-05_bs_16: Loss = 1.4526346921920776\nlr_3e-05_bs_8: Loss = 2.0304994583129883\nlr_3e-05_bs_16: Loss = 1.7399917840957642\nlr_5e-05_bs_8: Loss = 2.2076497077941895\nlr_5e-05_bs_16: Loss = 1.7654887437820435\nBest setup: ./best_model/lr_1e-05_bs_16 with loss = 1.4526346921920776\n","output_type":"stream"}]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('./best_model/lr_1e-05_bs_16')\n\ntraining_args = TrainingArguments(\n    output_dir=f'./results',\n    do_train=False,\n    do_predict=True\n)\n\ntrainer = Trainer(\n    model=best_model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\npredictions = trainer.predict(test_datasets)\n\n\nprint(predictions.metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T14:41:53.842218Z","iopub.execute_input":"2024-01-04T14:41:53.842675Z","iopub.status.idle":"2024-01-04T14:43:35.520038Z","shell.execute_reply.started":"2024-01-04T14:41:53.842640Z","shell.execute_reply":"2024-01-04T14:43:35.518889Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'test_loss': 1.4426931142807007, 'test_precision': 0.6157293889833815, 'test_recall': 0.6028984102365258, 'test_f1': 0.5942734150381467, 'test_accuracy': 0.6028984102365258, 'test_runtime': 101.4047, 'test_samples_per_second': 203.462, 'test_steps_per_second': 12.721}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}